{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmreGnd/Machine-Learning/blob/master/NLP_Sentiment_Analysis_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCgHlyCtyaZ0"
      },
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efAoxo_nyaaA"
      },
      "source": [
        "# WELCOME!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ussoVSxXyaaC"
      },
      "source": [
        "Welcome to the \"***Sentiment Analysis and Classification Project***\" project, the first and only project of the ***Natural Language Processing (NLP)*** course.\n",
        "\n",
        "This analysis will focus on using Natural Language techniques to find broad trends in the written thoughts of the customers. \n",
        "The goal in this project is to predict whether customers recommend the product they purchased using the information in their review text.\n",
        "\n",
        "One of the challenges in this project is to extract useful information from the *Review Text* variable using text mining techniques. The other challenge is that you need to convert text files into numeric feature vectors to run machine learning algorithms.\n",
        "\n",
        "At the end of this project, you will learn how to build sentiment classification models using Machine Learning algorithms (***Logistic Regression, Naive Bayes, Support Vector Machine, Random Forest*** and ***Ada Boosting***), **Deep Learning algorithms** and **BERT algorithm**.\n",
        "\n",
        "Before diving into the project, please take a look at the Determines and Tasks.\n",
        "\n",
        "- ***NOTE:*** *This tutorial assumes that you already know the basics of coding in Python and are familiar with the theory behind the algorithms mentioned above as well as NLP techniques.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDMyWVZKyaaF"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZoVuOBgyaaG"
      },
      "source": [
        "# #Determines\n",
        "The data is a collection of 22641 Rows and 10 column variables. Each row includes a written comment as well as additional customer information. \n",
        "Also each row corresponds to a customer review, and includes the variables:\n",
        "\n",
        "\n",
        "**Feature Information:**\n",
        "\n",
        "**Clothing ID:** Integer Categorical variable that refers to the specific piece being reviewed.\n",
        "\n",
        "**Age:** Positive Integer variable of the reviewers age.\n",
        "\n",
        "**Title:** String variable for the title of the review.\n",
        "\n",
        "**Review Text:** String variable for the review body.\n",
        "\n",
        "**Rating:** Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n",
        "\n",
        "**Recommended IND:** Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
        "\n",
        "**Positive Feedback Count:** Positive Integer documenting the number of other customers who found this review positive.\n",
        "\n",
        "**Division Name:** Categorical name of the product high level division.\n",
        "\n",
        "**Department Name:** Categorical name of the product department name.\n",
        "\n",
        "**Class Name:** Categorical name of the product class name.\n",
        "\n",
        "---\n",
        "\n",
        "The basic goal in this project is to predict whether customers recommend the product they purchased using the information in their *Review Text*.\n",
        "Especially, it should be noted that the expectation in this project is to use only the \"Review Text\" variable and neglect the other ones. \n",
        "Of course, if you want, you can work on other variables individually.\n",
        "\n",
        "Project Structure is separated in five tasks: ***EDA, Feature Selection and Data Cleaning , Text Mining, Word Cloud*** and ***Sentiment Classification with Machine Learning, Deep Learning and BERT model***.\n",
        "\n",
        "Classically, you can start to know the data after doing the import and load operations. \n",
        "You need to do missing value detection for Review Text, which is the only variable you need to care about. You can drop other variables.\n",
        "\n",
        "You will need to apply ***noise removal*** and ***lexicon normalization*** processes by using the capabilities of the ***nltk*** library to the data set that is ready for text mining.\n",
        "\n",
        "Afterwards, you will implement ***Word Cloud*** as a visual analysis of word repetition.\n",
        "\n",
        "Finally, You will build models with five different algorithms and compare their performance. Thus, you will determine the algorithm that makes the most accurate emotion estimation by using the information obtained from the * Review Text * variable.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1eKT-CuyaaJ"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os0DEWrcyaaK"
      },
      "source": [
        "# #Tasks\n",
        "\n",
        "#### 1. Exploratory Data Analysis\n",
        "\n",
        "- Import Modules, Load Discover the Data\n",
        "\n",
        "#### 2. Feature Selection and Data Cleaning\n",
        "\n",
        "- Feature Selection and Rename Column Name\n",
        "- Missing Value Detection\n",
        "\n",
        "#### 3. Text Mining\n",
        "\n",
        "- Tokenization\n",
        "- Noise Removal\n",
        "- Lexicon Normalization\n",
        "\n",
        "#### 4. WordCloud - Repetition of Words\n",
        "\n",
        "- Detect Reviews\n",
        "- Collect Words \n",
        "- Create Word Cloud \n",
        "\n",
        "\n",
        "#### 5. Sentiment Classification with Machine Learning, Deep Learning and BERT Model\n",
        "\n",
        "- Train - Test Split\n",
        "- Vectorization\n",
        "- TF-IDF\n",
        "- Logistic Regression\n",
        "- Naive Bayes\n",
        "- Support Vector Machine\n",
        "- Random Forest\n",
        "- AdaBoost\n",
        "- Deep Learning Model\n",
        "- BERT Model\n",
        "- Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ATtXzEyaaM"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOkP9vyYCV31"
      },
      "source": [
        "# Sentiment analysis of women's clothes reviews\n",
        "\n",
        "\n",
        "In this project we used sentiment analysis to determined whether the product is recommended or not. We used different machine learning algorithms to get more accurate predictions. The following classification algorithms have been used: ML algorithms(Logistic Regression, Naive Bayes, Support Vector Machine (SVM), Random Forest and Ada Boosting), Deep learning algorithm and BERT algorithm. The dataset comes from Woman Clothing Review that can be find at (https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkSHQgIGyaaP"
      },
      "source": [
        "## 1. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXzFEeYkCV32"
      },
      "source": [
        "### Import Libraries, Load and Discover the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xNQnqxE7yaaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccbe9d6f-348e-47ab-ef34-c156a4d9d43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib==3.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvDu_weCba9k",
        "outputId": "58e9b5a0-8415-4e53-9d91-04ae50216ead"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib==3.4 in /usr/local/lib/python3.7/dist-packages (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4) (1.21.6)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.4) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.4) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j9UnDZTfyaaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1715a47d-2f0e-46f5-cb2a-14c37a586f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Note that the `tpu` argument is for Colab-only\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "LDTXCDSbbHpD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "import nltk\n",
        "import matplotlib as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "d3DOx9iUbHzW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6HRpmw6kcde",
        "outputId": "f00df6e5-d0c0-484b-d47b-f16d5f88fbee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpvWMmIhbr23",
        "outputId": "cfab5fdb-c5b3-4c0c-e7f8-85e517b13916"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP/NLP sentiment project/Womens Clothing E-Commerce Reviews.csv\", index_col=0)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "23yAdYhlbvuM",
        "outputId": "0fcee993-0467-48a0-f6da-e94250af5803"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8b0e8456e11f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/NLP/NLP sentiment project/Womens Clothing E-Commerce Reviews.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NLP/NLP sentiment project/Womens Clothing E-Commerce Reviews.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T54GVJwICV36"
      },
      "source": [
        "### Data Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aS13LuaTyaaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "7fe22e44-f0bd-48aa-89c2-5bd1a4e80321"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d80377332f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Rating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Recommended IND\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize = (10,6))\n",
        "ax = sns.countplot(x=\"Rating\", data=df, hue = \"Recommended IND\")\n",
        "for p in ax.containers:\n",
        "    ax.bar_label(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7ZxywoyyaaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47bf2757-2de7-4b1b-bfb5-d2bf1ee0bb73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df[\"Recommended IND\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci-_fNfKfzRl",
        "outputId": "c4b804b2-0345-416a-e318-168e0c1c0e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23486"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df[\"Rating\"] == 1) & (df[\"Recommended IND\"] == 1)][\"Review Text\"].loc[5570]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Tg0VT_-LfzVQ",
        "outputId": "b06da803-d502-4566-d95c-d5039f1f8da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I love this t-shirt because it has the combination of elegance and relazation to a \"t\".\\r\\nit\\'s soft, pretty and covers my wide bottom well.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df[~((df[\"Rating\"] == 1) & (df[\"Recommended IND\"] == 1))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaXB7zObf1-L",
        "outputId": "8c88d498-e73b-4af7-e432-b380cc6d5678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23470"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~((df[\"Rating\"] == 1) & (df[\"Recommended IND\"] == 1))]\n",
        "df = df[~((df[\"Rating\"] == 2) & (df[\"Recommended IND\"] == 1))]\n",
        "df = df[~((df[\"Rating\"] == 3) & (df[\"Recommended IND\"] == 1))]\n",
        "df = df[~((df[\"Rating\"] == 4) & (df[\"Recommended IND\"] == 0))]\n",
        "df = df[~((df[\"Rating\"] == 5) & (df[\"Recommended IND\"] == 0))]"
      ],
      "metadata": {
        "id": "NFBWj0hAf2BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,6))\n",
        "ax = sns.countplot(x=\"Rating\", data=df, hue = \"Recommended IND\")\n",
        "for p in ax.containers:\n",
        "  ax.bar_label(p, label_type=\"center\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "luji71WDf2Ed",
        "outputId": "ab1fc82f-aaa8-4df8-e545-1641063066d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-137abde83cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Rating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Recommended IND\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"center\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQuhTpcwCV38"
      },
      "source": [
        "#### Check Proportion of Target Class Variable:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YXKZ5QgCV39"
      },
      "source": [
        "The target class variable is imbalanced, where \"Recommended\" values are more dominating then \"Not Recommendation\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX1Vq6KByaaZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "44d72df1-34ac-4a4f-87e8-90400a13ceae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-fb59c8e28b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Recommended IND\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "plt.fig = plt.figure(figsize = (10,6))\n",
        "ax = sns.countplot(x=\"Recommended IND\", data=df)\n",
        "ax.bar_label(ax.containers[0]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wqgNNr1yaaZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPEVV0JGCV37"
      },
      "source": [
        "## 2. Feature Selection and Data Cleaning\n",
        "\n",
        "From now on, the DataFrame you will work with should contain two columns: **\"Review Text\"** and **\"Recommended IND\"**. You can do the missing value detection operations from now on. You can also rename the column names if you want.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L06bPXRDyaaa"
      },
      "source": [
        "### Feature Selection and Rename Column Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odShfMvTyaab"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={\"Review Text\":\"text\", \"Recommended IND\":\"label\"}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzUwT6Alyaab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c2c736e7-5310-45b7-b5d1-98c8043cfba1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Clothing ID  Age                    Title  \\\n",
              "0          767   33                      NaN   \n",
              "1         1080   34                      NaN   \n",
              "2         1077   60  Some major design flaws   \n",
              "3         1049   50         My favorite buy!   \n",
              "4          847   47         Flattering shirt   \n",
              "\n",
              "                                                text  Rating  label  \\\n",
              "0  Absolutely wonderful - silky and sexy and comf...       4      1   \n",
              "1  Love this dress!  it's sooo pretty.  i happene...       5      1   \n",
              "2  I had such high hopes for this dress and reall...       3      0   \n",
              "3  I love, love, love this jumpsuit. it's fun, fl...       5      1   \n",
              "4  This shirt is very flattering to all due to th...       5      1   \n",
              "\n",
              "   Positive Feedback Count   Division Name Department Name Class Name  \n",
              "0                        0       Initmates        Intimate  Intimates  \n",
              "1                        4         General         Dresses    Dresses  \n",
              "2                        0         General         Dresses    Dresses  \n",
              "3                        0  General Petite         Bottoms      Pants  \n",
              "4                        6         General            Tops    Blouses  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3bcb3c49-dc48-43ac-ba29-be268204bbbb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clothing ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Title</th>\n",
              "      <th>text</th>\n",
              "      <th>Rating</th>\n",
              "      <th>label</th>\n",
              "      <th>Positive Feedback Count</th>\n",
              "      <th>Division Name</th>\n",
              "      <th>Department Name</th>\n",
              "      <th>Class Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>767</td>\n",
              "      <td>33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Initmates</td>\n",
              "      <td>Intimate</td>\n",
              "      <td>Intimates</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1080</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1077</td>\n",
              "      <td>60</td>\n",
              "      <td>Some major design flaws</td>\n",
              "      <td>I had such high hopes for this dress and reall...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>General</td>\n",
              "      <td>Dresses</td>\n",
              "      <td>Dresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1049</td>\n",
              "      <td>50</td>\n",
              "      <td>My favorite buy!</td>\n",
              "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>General Petite</td>\n",
              "      <td>Bottoms</td>\n",
              "      <td>Pants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>847</td>\n",
              "      <td>47</td>\n",
              "      <td>Flattering shirt</td>\n",
              "      <td>This shirt is very flattering to all due to th...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>General</td>\n",
              "      <td>Tops</td>\n",
              "      <td>Blouses</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bcb3c49-dc48-43ac-ba29-be268204bbbb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3bcb3c49-dc48-43ac-ba29-be268204bbbb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3bcb3c49-dc48-43ac-ba29-be268204bbbb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[['text','label']]\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MdFrLf0Rgbr0",
        "outputId": "2ffae6aa-d394-4ce6-807b-6ec3768c8a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  Absolutely wonderful - silky and sexy and comf...      1\n",
              "1  Love this dress!  it's sooo pretty.  i happene...      1\n",
              "2  I had such high hopes for this dress and reall...      0\n",
              "3  I love, love, love this jumpsuit. it's fun, fl...      1\n",
              "4  This shirt is very flattering to all due to th...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc222fdb-3235-4d31-a1b8-aed6d231dc82\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I had such high hopes for this dress and reall...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This shirt is very flattering to all due to th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc222fdb-3235-4d31-a1b8-aed6d231dc82')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc222fdb-3235-4d31-a1b8-aed6d231dc82 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc222fdb-3235-4d31-a1b8-aed6d231dc82');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxT4Uo_eyaac"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl2h73vYCV38"
      },
      "source": [
        "### Missing Value Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5po56Cqmyaad"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDw7hf8qyaad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "442017a4-9789-4877-eaac-c0f357d1e555"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  label\n",
              "0      Absolutely wonderful - silky and sexy and comf...      1\n",
              "1      Love this dress!  it's sooo pretty.  i happene...      1\n",
              "2      I had such high hopes for this dress and reall...      0\n",
              "3      I love, love, love this jumpsuit. it's fun, fl...      1\n",
              "4      This shirt is very flattering to all due to th...      1\n",
              "...                                                  ...    ...\n",
              "21170  I was surprised at the positive reviews for th...      0\n",
              "21171  So i wasn't sure about ordering this skirt bec...      1\n",
              "21172  I was very happy to snag this dress at such a ...      1\n",
              "21173  This fit well, but the top was very see throug...      0\n",
              "21174  This dress in a lovely platinum is feminine an...      1\n",
              "\n",
              "[21175 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cdd26df5-3402-4850-855e-c9574319c74b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I had such high hopes for this dress and reall...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This shirt is very flattering to all due to th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21170</th>\n",
              "      <td>I was surprised at the positive reviews for th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21171</th>\n",
              "      <td>So i wasn't sure about ordering this skirt bec...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21172</th>\n",
              "      <td>I was very happy to snag this dress at such a ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21173</th>\n",
              "      <td>This fit well, but the top was very see throug...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21174</th>\n",
              "      <td>This dress in a lovely platinum is feminine an...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21175 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdd26df5-3402-4850-855e-c9574319c74b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cdd26df5-3402-4850-855e-c9574319c74b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cdd26df5-3402-4850-855e-c9574319c74b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df.dropna(inplace = True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.fig = plt.figure(figsize = (10,6))\n",
        "ax = sns.countplot(x=\"label\", data=df)\n",
        "ax.bar_label(ax.containers[0]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "y0Y7Mp-NggQS",
        "outputId": "56d61010-f603-46a2-e129-f7bb32d0057f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3a3021c16afa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9qD9OwUyaae"
      },
      "source": [
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO_zLq2UCV39"
      },
      "source": [
        "## 3. Text Mining\n",
        "\n",
        "Text is the most unstructured form of all the available data, therefore various types of noise are present in it. This means that the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as **text preprocessing**.\n",
        "\n",
        "The three key steps of text preprocessing:\n",
        "\n",
        "- **Tokenization:**\n",
        "This step is one of the top priorities when it comes to working on text mining. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
        "\n",
        "- **Noise Removal:**\n",
        "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
        "For example – language stopwords (commonly used words of a language – is, am, the, of, in etc), URLs or links, upper and lower case differentiation, punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n",
        "\n",
        "\n",
        "- **Lexicon Normalization:**\n",
        "Another type of textual noise is about the multiple representations exhibited by single word.\n",
        "For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”. Though they mean different things, contextually they all are similar. This step converts all the disparities of a word into their normalized form (also known as lemma). \n",
        "There are two methods of lexicon normalisation; **[Stemming or Lemmatization](https://www.guru99.com/stemming-lemmatization-python-nltk.html)**. Lemmatization is recommended for this case, because Lemmatization as this will return the root form of each word (rather than just stripping suffixes, which is stemming).\n",
        "\n",
        "As the first step change text to tokens and convertion all of the words to lower case.  Next remove punctuation, bad characters, numbers and stop words. The second step is aimed to normalization them throught the Lemmatization method. \n",
        "\n",
        "\n",
        "***Note:*** *Use the functions of the ***[nltk Library](https://www.guru99.com/nltk-tutorial.html)*** for all the above operations.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7TYkFEyaaf"
      },
      "source": [
        "### Tokenization, Noise Removal, Lexicon Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bbvWQE_yaah"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "for i in [\"not\", \"no\"]:\n",
        "        stop_words.remove(i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning(data):\n",
        "    \n",
        "    #1. Tokenize\n",
        "    text_tokens = word_tokenize(data.replace(\"'\", \"\").lower())\n",
        "    \n",
        "    #2. Remove Puncs and numbers\n",
        "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
        "    \n",
        "    #3. Removing Stopwords\n",
        "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
        "    \n",
        "    #4. lemma\n",
        "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
        "    \n",
        "    #joining\n",
        "    return \" \".join(text_cleaned)"
      ],
      "metadata": {
        "id": "bkDCC6Zfqh-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DTNRasaCV3-"
      },
      "source": [
        "## 4. WordCloud - Repetition of Words\n",
        "\n",
        "Now you'll create a Word Clouds for reviews, representing most common words in each target class.\n",
        "\n",
        "Word Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. Significant textual data points can be highlighted using a word cloud.\n",
        "\n",
        "You are expected to create separate word clouds for positive and negative reviews. You can qualify a review as positive or negative, by looking at its recommended status. You may need to use capabilities of matplotlib for visualizations.\n",
        "\n",
        "You can follow the steps below:\n",
        "\n",
        "- Detect Reviews\n",
        "- Collect Words \n",
        "- Create Word Cloud \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZoWiF7Lyaaj"
      },
      "source": [
        "### Detect Reviews (positive and negative separately)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO6z4kUeyaak"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WEAXfPxyaal"
      },
      "outputs": [],
      "source": [
        "positive_sentences = df[df[\"label\"] == 1][\"text\"]\n",
        "positive_sentences = positive_sentences.apply(cleaning)\n",
        "positive_sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_sentences = df[df[\"label\"] == 0][\"text\"]\n",
        "negative_sentences = negative_sentences.apply(cleaning)\n",
        "negative_sentences"
      ],
      "metadata": {
        "id": "RPZwNfaCqstK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WsF_3Zmyaam"
      },
      "source": [
        "### Collect Words (positive and negative separately)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sgKO679yaam"
      },
      "outputs": [],
      "source": [
        "positive_words = \" \".join(positive_sentences)\n",
        "positive_words[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo-wuz3Wyaam"
      },
      "outputs": [],
      "source": [
        "negative_words = \" \".join(negative_sentences)\n",
        "negative_words[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq8RVsr2yaan"
      },
      "source": [
        "### Create Word Cloud (for most common words in recommended not recommended reviews separately)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgwbzDWKyaan"
      },
      "outputs": [],
      "source": [
        "rom wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XxHfK43yaan"
      },
      "outputs": [],
      "source": [
        "wordcloud_positive = WordCloud(background_color=\"white\", max_words =250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHCH7RuXyaan"
      },
      "outputs": [],
      "source": [
        "wordcloud_positive.generate(positive_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (13,13))\n",
        "plt.imshow(wordcloud_positive, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8kLpv_81qzQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud_negative = WordCloud(background_color=\"white\", max_words=250, colormap='gist_heat')\n",
        "\n",
        "wordcloud_negative.generate(negative_words)\n",
        "\n",
        "plt.figure(figsize=(13,13))\n",
        "plt.imshow(wordcloud_negative, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fE5Bi3tAq1EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FftUXU2Rq1G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9_e1A26CV3_"
      },
      "source": [
        "## 5. Sentiment Classification with Machine Learning, Deep Learning and BERT model\n",
        "\n",
        "Before moving on to modeling, as data preprocessing steps you will need to perform **[vectorization](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)** and **train-test split**. You have performed many times train test split process before.\n",
        "But you will perform the vectorization for the first time.\n",
        "\n",
        "Machine learning algorithms most often take numeric feature vectors as input. Thus, when working with text documents, you need a way to convert each document into a numeric vector. This process is known as text vectorization. Commonly used vectorization approach that you will use here is to represent each text as a vector of word counts.\n",
        "\n",
        "At this moment, you have your review text column as a token (which has no punctuations and stopwords). You can use Scikit-learn’s CountVectorizer to convert the text collection into a matrix of token counts. You can imagine this resulting matrix as a 2-D matrix, where each row is a unique word, and each column is a review.\n",
        "\n",
        "Train all models using TFIDF and Count vectorizer data.\n",
        "\n",
        "**For Deep learning model, use embedding layer for all words.** \n",
        "\n",
        "**For BERT model, use TF tensor**\n",
        "\n",
        "After performing data preprocessing, build your models using following classification algorithms:\n",
        "\n",
        "- Logistic Regression,\n",
        "- Naive Bayes,\n",
        "- Support Vector Machine,\n",
        "- Random Forest,\n",
        "- Ada Boosting\n",
        "- Deep Learning Model\n",
        "- BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qZC05fWyaao"
      },
      "source": [
        "### Train - Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2SMUUrxCV3_"
      },
      "source": [
        "To run machine learning algorithms we need to convert text files into numerical feature vectors. We will use bag of words model for our analysis.\n",
        "\n",
        "First we spliting the data into train and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5jiM8T8yaap"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zljupyBkyaap"
      },
      "outputs": [],
      "source": [
        "X = df[\"text\"].values\n",
        "y = df[\"label\"].map({0:1, 1:0}).values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=101)"
      ],
      "metadata": {
        "id": "Cj1od9wHq7BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ob04po8oq7Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adhWeL_iCV3_"
      },
      "source": [
        "In the next step we create a numerical feature vector for each document:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMPV90pbyaaq"
      },
      "source": [
        "### Count Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8idORUOyaaq"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y67aO62yaar"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(preprocessor=cleaning, min_df=3)\n",
        "X_train_count = vectorizer.fit_transform(X_train)\n",
        "X_test_count = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_count.toarray()"
      ],
      "metadata": {
        "id": "xgcG98Imq__x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "9u1KXbO_rACf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qu3D94P3rAEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hI_P6_Ryaar"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSSP6ZJkyaar"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23B1N2XSyaas"
      },
      "outputs": [],
      "source": [
        "tf_idf_vectorizer = TfidfVectorizer(preprocessor=cleaning, min_df=3)\n",
        "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
        "X_test_tf_idf = tf_idf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tf_idf.toarray()"
      ],
      "metadata": {
        "id": "sZYDPLBxrDDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "P89c3GrSrDFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval functions\n"
      ],
      "metadata": {
        "id": "eelCAiJ-rJbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report, f1_score, recall_score, accuracy_score, precision_score"
      ],
      "metadata": {
        "id": "TrZtHeyerMC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, X_train, X_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"Test_Set\")\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print(\"Train_Set\")\n",
        "    print(classification_report(y_train,y_pred_train))"
      ],
      "metadata": {
        "id": "qhNu741JrNgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaVrkxRpCV3_"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbVbTgoYyaas"
      },
      "source": [
        "### CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL84xDUfyaat"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log = LogisticRegression(C =0.01, max_iter=1000, class_weight= \"balanced\", random_state=101)\n",
        "log.fit(X_train_count,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8QC0uBYyaat"
      },
      "outputs": [],
      "source": [
        "print(\"LOG MODEL\")\n",
        "eval(log, X_train_count, X_test_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdYd2pAQyaat"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "scoring= [\"accuracy\", \"f1\", \"recall\", \"precision\"]\n",
        "\n",
        "model = LogisticRegression(C =0.01, max_iter=1000, class_weight= \"balanced\", random_state=101)\n",
        "scores = cross_validate(model, X_train_count, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import PrecisionRecallCurve\n",
        "viz = PrecisionRecallCurve(\n",
        "    LogisticRegression(C =0.01, max_iter=1000, class_weight= \"balanced\", random_state=101),\n",
        "    classes=log.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_count,y_train)\n",
        "viz.score(X_test_count, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "QaabPUDzrTSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = log.predict(X_test_count)\n",
        "log_AP_count = viz.score_\n",
        "log_count_rec = recall_score(y_test, y_pred)\n",
        "log_count_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "RUIcc6syrTVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCVIPEEfyaat"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGLZC3pXyaau"
      },
      "outputs": [],
      "source": [
        "log = LogisticRegression(C=0.06, max_iter=1000, random_state=101, class_weight=\"balanced\")\n",
        "log.fit(X_train_tf_idf,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqWSaoNUyaau"
      },
      "outputs": [],
      "source": [
        "print(\"LOG MODEL\")\n",
        "eval(log, X_train_tf_idf, X_test_tf_idf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(C=0.06, max_iter=1000, random_state=101, class_weight=\"balanced\")\n",
        "scores = cross_validate(model, X_train_tf_idf, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "Zg6UzbwAraMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    LogisticRegression(C=0.06, max_iter=1000, random_state=101, class_weight=\"balanced\"),\n",
        "    classes=log.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_tf_idf,y_train)\n",
        "viz.score(X_test_tf_idf, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "DO8y-0ZkraUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = log.predict(X_test_tf_idf)\n",
        "log_AP_tfidf = viz.score_\n",
        "log_tfidf_rec = recall_score(y_test, y_pred)\n",
        "log_tfidf_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "2zyGhjODrc60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QACRU9G3CV4A"
      },
      "source": [
        "## Naive Bayes \n",
        "\n",
        "### Countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE0i90Q5yaav"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "nb = MultinomialNB(alpha=0.7)\n",
        "nb.fit(X_train_count,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRGX7Ck6yaaw"
      },
      "outputs": [],
      "source": [
        "print(\"NB MODEL\")\n",
        "eval(nb, X_train_count, X_test_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNB(alpha=0.7)\n",
        "scores = cross_validate(model, X_train_count, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "nUSy5Q9brihn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    MultinomialNB(alpha=0.7),\n",
        "    classes=nb.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_count,y_train)\n",
        "viz.score(X_test_count, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "ilveSe5Nrikq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb.predict(X_test_count)\n",
        "nb_AP_count = viz.score_\n",
        "nb_count_rec = recall_score(y_test, y_pred)\n",
        "nb_count_f1 = f1_score(y_test,y_pred)\n",
        "TF-IDF"
      ],
      "metadata": {
        "id": "3yAORxgErind"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0YPoHQIyaaw"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH34zsJqyaax"
      },
      "outputs": [],
      "source": [
        "nb = BernoulliNB(alpha=0.6)\n",
        "nb.fit(X_train_tf_idf,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rWCvkHmyaay"
      },
      "outputs": [],
      "source": [
        "print(\"NB MODEL\")\n",
        "eval(nb, X_train_tf_idf, X_test_tf_idf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BernoulliNB(alpha=0.6)\n",
        "scores = cross_validate(model, X_train_tf_idf, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "_pYt4Nfyrpj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    BernoulliNB(alpha=0.6),\n",
        "    classes=nb.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_tf_idf,y_train)\n",
        "viz.score(X_test_tf_idf, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "mjN0_ov8rpm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb.predict(X_test_tf_idf)\n",
        "nb_AP_tfidf = viz.score_\n",
        "nb_tfidf_rec = recall_score(y_test, y_pred)\n",
        "nb_tfidf_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "veCGmFKjrpr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSkbnJJiCV4A"
      },
      "source": [
        "## Support Vector Machine (SVM)\n",
        "\n",
        "### Countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP-Io7Y4yaa0"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "svc = LinearSVC(C=0.001, class_weight=\"balanced\", random_state=101)\n",
        "svc.fit(X_train_count,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-4GHQYWyaa1"
      },
      "outputs": [],
      "source": [
        "print(\"SVC MODEL\")\n",
        "eval(svc, X_train_count, X_test_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSVC(C=0.001, class_weight=\"balanced\", random_state=101)\n",
        "scores = cross_validate(model, X_train_count, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "u-wxrD1vrwFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    LinearSVC(C=0.001, class_weight=\"balanced\", random_state=101),\n",
        "    classes=svc.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_count,y_train)\n",
        "viz.score(X_test_count, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "5-Xeo_YLrwIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svc.predict(X_test_count)\n",
        "svc_AP_count = viz.score_\n",
        "svc_count_rec = recall_score(y_test, y_pred)\n",
        "svc_count_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "nYBuFUE6rwNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-TtYx3YrwRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hYtFwdNrwUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAX3oHiKyaa2"
      },
      "source": [
        "### TD-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kvmchzafyaa3"
      },
      "outputs": [],
      "source": [
        "svc = LinearSVC(C=0.03, class_weight=\"balanced\", random_state=101)\n",
        "svc.fit(X_train_tf_idf,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XitL-5Iyaa4"
      },
      "outputs": [],
      "source": [
        "print(\"SVC MODEL\")\n",
        "eval(svc, X_train_tf_idf, X_test_tf_idf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSVC(C=0.03, class_weight=\"balanced\", random_state=101)\n",
        "scores = cross_validate(model, X_train_tf_idf, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "RGd6xCp2r3Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    LinearSVC(C=0.03, class_weight=\"balanced\", random_state=101),\n",
        "    classes=svc.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_tf_idf,y_train)\n",
        "viz.score(X_test_tf_idf, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "xODCyhFKr3Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svc.predict(X_test_tf_idf)\n",
        "svc_AP_tfidf = viz.score_\n",
        "svc_tfidf_rec = recall_score(y_test, y_pred)\n",
        "svc_tfidf_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "Ae8F36gJr5mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTECEchfCV4A"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "### Countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E4GTelvyaa5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(100, max_depth = 9, random_state = 42, n_jobs = -1, class_weight=\"balanced\")\n",
        "rf.fit(X_train_count, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oukKdl74yaa5"
      },
      "outputs": [],
      "source": [
        "print(\"RF MODEL\")\n",
        "eval(rf, X_train_count, X_test_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(100, max_depth = 9, random_state = 42, n_jobs = -1, class_weight=\"balanced\")\n",
        "scores = cross_validate(model, X_train_count, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "dQyMXhOhr_7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    RandomForestClassifier(100, max_depth = 9, random_state = 42, n_jobs = -1, class_weight=\"balanced\"),\n",
        "    classes=rf.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_count,y_train)\n",
        "viz.score(X_test_count, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "DiEru7jUr_-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf.predict(X_test_count)\n",
        "rf_AP_count = viz.score_\n",
        "rf_count_rec = recall_score(y_test, y_pred)\n",
        "rf_count_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "QwY7O8f5sAET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXtEODdFyaa6"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5EfAthhyaa6"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(100, max_depth = 9, random_state = 42, n_jobs = -1, class_weight=\"balanced\")\n",
        "rf.fit(X_train_tf_idf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO3OW5xayaa6"
      },
      "outputs": [],
      "source": [
        "print(\"RF MODEL\")\n",
        "eval(rf, X_train_tf_idf, X_test_tf_idf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(100, max_depth = 9, random_state = 42, n_jobs = -1, class_weight=\"balanced\")\n",
        "scores = cross_validate(model, X_train_tf_idf, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "ykuVwt1DsIz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    RandomForestClassifier(100, max_depth = 9, random_state = 42, n_jobs = -1, class_weight=\"balanced\"),\n",
        "    classes=rf.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_tf_idf,y_train)\n",
        "viz.score(X_test_tf_idf, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "FFZPYkfesI3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf.predict(X_test_tf_idf)\n",
        "rf_AP_tfidf = viz.score_\n",
        "rf_tfidf_rec = recall_score(y_test, y_pred)\n",
        "rf_tfidf_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "LjSs2UNwsI64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SezeeY2yaa6"
      },
      "source": [
        "## Ada Boosting\n",
        "\n",
        "### Countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh_3SQPuyaa7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "ada = AdaBoostClassifier(n_estimators= 500, random_state = 42, learning_rate=0.8)\n",
        "ada.fit(X_train_count, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAl3ZHO9yaa7"
      },
      "outputs": [],
      "source": [
        "print(\"Ada MODEL\")\n",
        "eval(ada, X_train_count, X_test_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AdaBoostClassifier(n_estimators= 500, random_state = 42, learning_rate=0.8)\n",
        "scores = cross_validate(model, X_train_count, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "oo_FhA_0sRTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    AdaBoostClassifier(n_estimators= 500, random_state = 42, learning_rate=0.8),\n",
        "    classes=ada.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_count,y_train)\n",
        "viz.score(X_test_count, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "7Mj6Lm-RsRW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ada.predict(X_test_count)\n",
        "ada_AP_count = viz.score_\n",
        "ada_count_rec = recall_score(y_test, y_pred)\n",
        "ada_count_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "FUvKgXyOsR_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkBYO2fdyaa8"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO-PdyCtyaa8"
      },
      "outputs": [],
      "source": [
        "ada = AdaBoostClassifier(n_estimators= 200, random_state = 42, learning_rate=0.8)\n",
        "ada.fit(X_train_tf_idf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eLEu7-Nyaa8"
      },
      "outputs": [],
      "source": [
        "print(\"Ada MODEL\")\n",
        "eval(ada, X_train_tf_idf, X_test_tf_idf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AdaBoostClassifier(n_estimators= 200, random_state = 42, learning_rate=0.8)\n",
        "scores = cross_validate(model, X_train_tf_idf, y_train, scoring = scoring, cv = 10)\n",
        "df_scores = pd.DataFrame(scores, index = range(1, 11))\n",
        "df_scores.mean()[2:]"
      ],
      "metadata": {
        "id": "mQ9EApZUsa5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz = PrecisionRecallCurve(\n",
        "    AdaBoostClassifier(n_estimators= 200, random_state = 42, learning_rate=0.8),\n",
        "    classes=ada.classes_,\n",
        "    per_class=True,\n",
        "    cmap=\"Set1\"\n",
        ")\n",
        "viz.fit(X_train_tf_idf,y_train)\n",
        "viz.score(X_test_tf_idf, y_test)\n",
        "viz.show();"
      ],
      "metadata": {
        "id": "F8S84jeosbAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ada.predict(X_test_tf_idf)\n",
        "ada_AP_tfidf = viz.score_\n",
        "ada_tfidf_rec = recall_score(y_test, y_pred)\n",
        "ada_tfidf_f1 = f1_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "iTSSJV3SsbHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_u6smUMyaa8"
      },
      "source": [
        "## DL modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L41zsZXfyaa9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgZzYhYfyaa9"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "zel78zL0shSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsBCaOJwyaa9"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VoMw3wWyaa-"
      },
      "outputs": [],
      "source": [
        "num_words = 15000 # corpusta geçen en fazla kullanılan ilk 15.000 kelimeyi(tokeni) alacağız gerisini yok sayacağız. \n",
        "# None değeri atanırsa tüm kelimeler dikkate alınır.\n",
        "tokenizer = Tokenizer(num_words=num_words) #filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n1234567890' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk9qU4g3yaa-"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBqiLAauyaa_"
      },
      "source": [
        "### Creating word index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2-nask6yabA"
      },
      "outputs": [],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozBK5t7_yabB"
      },
      "outputs": [],
      "source": [
        "len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BgD95SgyabC"
      },
      "source": [
        "### Converting tokens to numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgyYCVMeyabD"
      },
      "outputs": [],
      "source": [
        "X_num_tokens = tokenizer.texts_to_sequences(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L6LpsuqyabE"
      },
      "outputs": [],
      "source": [
        "X[100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_num_tokens[100])"
      ],
      "metadata": {
        "id": "TTQmyVDRswEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj9JkageyabE"
      },
      "source": [
        "### Maximum number of tokens for all documents¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEGs4NkGyabF"
      },
      "outputs": [],
      "source": [
        "len(X_num_tokens[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpHWEnkvyabG"
      },
      "outputs": [],
      "source": [
        "len(X_num_tokens[150])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = [len(tokens) for tokens in X_num_tokens]\n",
        "num_tokens = np.array(num_tokens)"
      ],
      "metadata": {
        "id": "ZS5zxH65s22G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens.mean()"
      ],
      "metadata": {
        "id": "jxQN7WCGs25x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens.max()"
      ],
      "metadata": {
        "id": "mNeU2X1Vs2-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens.argmax()"
      ],
      "metadata": {
        "id": "Ft8XB54ys70I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[15213]"
      ],
      "metadata": {
        "id": "bgIjdXSPs77B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 116"
      ],
      "metadata": {
        "id": "gmgjOfw9s8HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(num_tokens <= max_tokens) / len(num_tokens)"
      ],
      "metadata": {
        "id": "1BqJmgJLs_gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy1B4hK-yabG"
      },
      "source": [
        "### Fixing token counts of all documents (pad_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iWx4UIMyabH"
      },
      "outputs": [],
      "source": [
        "X_pad = pad_sequences(X_num_tokens, maxlen=max_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dhsOUyPyabI"
      },
      "outputs": [],
      "source": [
        "X_pad.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(X_num_tokens[500])"
      ],
      "metadata": {
        "id": "102tL6tetElZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pad[500]"
      ],
      "metadata": {
        "id": "2EyoS6U3tFt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(X_num_tokens[15213])"
      ],
      "metadata": {
        "id": "MdZVLG20tFxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pad[15213]"
      ],
      "metadata": {
        "id": "Ojy9j3RptF2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDncmu1XyabI"
      },
      "source": [
        "### Train Set Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqsRIshXyabJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0S3dNtLyabJ"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.1, stratify=y, random_state=101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNgzp1YlyabJ"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coscTq6fyabK"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB4exQP0yabK"
      },
      "outputs": [],
      "source": [
        "embedding_size = 50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    input_length=max_tokens))\n",
        "\n",
        "\n",
        "model.add(GRU(units=48, return_sequences=True))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(GRU(units=24, return_sequences=True))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(GRU(units=24))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "AHgS2timtMeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(lr=0.001)"
      ],
      "metadata": {
        "id": "z6m3FovrtMib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"Recall\"])"
      ],
      "metadata": {
        "id": "rcbIbdBQtMnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary() \n",
        "# 3 x (n2 + nm + 2n) m= input, n= output\n",
        "# (48*48 + 100*48 +2*48)*3 ==> m = 100, n = 48\n",
        "# (48*48 + 48*48  +2*48)*3 ==> m =  48, n = 48\n",
        "# (48*48 + 48*48  +2*48)*3 ==> m =  48, n = 48"
      ],
      "metadata": {
        "id": "eiUpaL3TtUUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor=\"val_recall\", mode=\"max\", \n",
        "                           verbose=1, patience = 1, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "BVMRfjfLtUXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "classes_weights = class_weight.compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "pd.Series(classes_weights).unique()"
      ],
      "metadata": {
        "id": "VX6t9BCPtXWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=25, batch_size=128, sample_weight=classes_weights,\n",
        "         validation_data=(X_test, y_test), callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "J_UcdRYWtXZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90psEuqfyabK"
      },
      "source": [
        "## BERT Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jOXjgWtyabL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux_iaGqlyabL"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zApTMyZAyabM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti_HMgv-O0hP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "max_token = []\n",
        "for sent in X:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    \n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_token.append(len(input_ids))\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(max_token).mean()"
      ],
      "metadata": {
        "id": "lOLAm5bptlpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(np.array(max_token) <= 162) / len(max_token)"
      ],
      "metadata": {
        "id": "RMryh_xbtlty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformation(X):\n",
        "  # set array dimensions\n",
        "  seq_len = 162\n",
        "  num_samples = len(X)\n",
        "\n",
        "  # initialize empty zero arrays\n",
        "  Xids = np.zeros((num_samples, seq_len))\n",
        "  Xmask = np.zeros((num_samples, seq_len))\n",
        "\n",
        "    \n",
        "  for i, phrase in enumerate(X):\n",
        "      tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True,\n",
        "                                      padding='max_length', add_special_tokens=True) \n",
        "      # assign tokenized outputs to respective rows in numpy arrays\n",
        "      Xids[i, :] = tokens['input_ids']\n",
        "      Xmask[i, :] = tokens['attention_mask']\n",
        "  return Xids, Xmask"
      ],
      "metadata": {
        "id": "yhaWFlEhtoTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xids, Xmask = transformation(X)"
      ],
      "metadata": {
        "id": "cnL3rHNdtoYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xids.shape"
      ],
      "metadata": {
        "id": "6kiZf_jhtrXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xmask.shape"
      ],
      "metadata": {
        "id": "SGTj9Ekctrbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = y.reshape(-1,1)\n",
        "labels"
      ],
      "metadata": {
        "id": "qlr6XlBAtrjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DRmOt6kO2Cr"
      },
      "source": [
        "### Train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMh01UkEyabM"
      },
      "outputs": [],
      "source": [
        "# set split size (90% training data) and calculate training set size\n",
        "split = 0.9\n",
        "size = int(len(dataset)*split) #int((Xids.shape[0]/batch_size)*split)\n",
        "\n",
        "# get training and validation sets\n",
        "train_ds = dataset.take(size)\n",
        "val_ds = dataset.skip(size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pu5teiDyabM"
      },
      "source": [
        "### Transformation text to tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjufdfhryabN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels)) #tf.data.Dataset.from_tensors((Xids, Xmask, labels))\n",
        "\n",
        "def map_func(Xids, Xmask, labels):\n",
        "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
        "    return {'input_ids': Xids, 'attention_mask': Xmask}, labels\n",
        "\n",
        "# then we use the dataset map method to apply this transformation\n",
        "dataset = dataset.map(map_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7UcEY1syabN"
      },
      "outputs": [],
      "source": [
        "len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpREU78cyabW"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "length = len(X)\n",
        "# shuffle and batch\n",
        "# fit into a batch of 16\n",
        "dataset = dataset.shuffle(length, reshuffle_each_iteration=False).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8c-hlqCPJY2"
      },
      "source": [
        "### Batch Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QPfdcqdyabX"
      },
      "source": [
        "### Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNt0Mo6QyabX"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    from transformers import TFAutoModel\n",
        "    model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "    input_ids = tf.keras.layers.Input(shape=(162,), name='input_ids', dtype='int32')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(162,), name='attention_mask', dtype='int32')\n",
        "\n",
        "    embeddings = model.bert(input_ids=input_ids, attention_mask=attention_mask)[\"pooler_output\"] #[1]\n",
        "\n",
        "    x = tf.keras.layers.Dense(80, activation='relu')(embeddings) \n",
        "    x = tf.keras.layers.Dropout(0.1, name=\"dropout\")(x) #0.1\n",
        "    y = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEtMIXv2yabX"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  \n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5) #3e-5, 5e-5\n",
        "  loss = tf.keras.losses.BinaryCrossentropy()\n",
        "  recall = tf.keras.metrics.Recall()\n",
        "  model3 = create_model()\n",
        "  model3.compile(optimizer=optimizer, loss=loss, metrics=[recall])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCyJKZ8IyabY"
      },
      "outputs": [],
      "source": [
        "model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrC2_jJsyabY"
      },
      "outputs": [],
      "source": [
        "history = model3.fit(\n",
        "    train_ds, validation_data= val_ds,\n",
        "    epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1ffMrzzyaba"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veJ3S964yabb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = model3.predict(val_ds) >= 0.5\n",
        "\n",
        "y_test = [j for i in val_ds for j in np.array(i[1])]\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6wnuC81yabb"
      },
      "outputs": [],
      "source": [
        "y_train_pred = model3.predict(train_ds) >= 0.5\n",
        "\n",
        "y_train = [j for i in train_ds for j in np.array(i[1])]\n",
        "\n",
        "print(classification_report(y_train, y_train_pred)) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_proba = model3.predict(val_ds)\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.plot([1,0],[0,1],'k--')\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('recall')\n",
        "plt.ylabel('precision')\n",
        "plt.title('precision recall curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BoI323cet-Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_precision_score(y_test, y_pred_proba)\n"
      ],
      "metadata": {
        "id": "Zc_EV11Kt-h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_without_weighted_AP = average_precision_score(y_test, y_pred_proba)\n",
        "BERT_without_weighted_f1 = f1_score(y_test, y_pred)\n",
        "BERT_without_weighted_rec = recall_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "HwUIuuJquBOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdc5JYOXCV4A"
      },
      "source": [
        "### Compare Models F1 Scores, Recall Scores and Average Precision Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0NYuHKNyabd"
      },
      "outputs": [],
      "source": [
        "compare = pd.DataFrame({\"Model\": [\"NaiveBayes_count\", \"LogReg_count\", \"SVM_count\", \"Random Forest_count\", \n",
        "                                  \"AdaBoost_count\", \"NaiveBayes_tfidf\", \"LogReg_tfidf\", \"SVM_tfidf\", \n",
        "                                  \"Random Forest_tfidf\", \"AdaBoost_tfidf\", \"DL\", \"BERT_Weighted\", \"BERT_Without_Weighted\"],\n",
        "                        \n",
        "                        \"F1_Score\": [nb_count_f1, log_count_f1, svc_count_f1,\n",
        "                                             rf_count_f1, ada_count_f1, nb_tfidf_f1, log_tfidf_f1,\n",
        "                                             svc_tfidf_f1, rf_tfidf_f1, ada_tfidf_f1, DL_f1, BERT_with_weighted_f1, BERT_without_weighted_f1],\n",
        "                        \n",
        "                        \"Recall_Score\": [nb_count_rec, log_count_rec, svc_count_rec, \n",
        "                                                   rf_count_rec, ada_count_rec, \n",
        "                                                  nb_tfidf_rec, log_tfidf_rec, svc_tfidf_rec, \n",
        "                                                  rf_tfidf_rec, ada_tfidf_rec, DL_rec, BERT_with_weighted_rec, BERT_without_weighted_rec],\n",
        "                        \n",
        "                        \"Average_Precision_Score\": [nb_AP_count, log_AP_count, svc_AP_count, rf_AP_count,\n",
        "                                                   ada_AP_count, nb_AP_tfidf, log_AP_tfidf, svc_AP_tfidf,\n",
        "                                                   rf_AP_tfidf, ada_AP_tfidf, DL_AP, BERT_with_weighted_AP, BERT_without_weighted_AP]})\n",
        "\n",
        "def labels(ax):\n",
        "                        \n",
        "     for p in ax.patches:\n",
        "        ax.bar_label(ax.containers[0],fmt=\"%.3f\")\n",
        "    \n",
        "plt.figure(figsize=(15,30))\n",
        "plt.subplot(311)\n",
        "compare = compare.sort_values(by=\"Recall_Score\", ascending=False)\n",
        "ax=sns.barplot(x=\"Recall_Score\", y=\"Model\", data=compare, palette=\"Blues_d\")\n",
        "labels(ax)\n",
        "\n",
        "plt.subplot(312)\n",
        "compare = compare.sort_values(by=\"F1_Score\", ascending=False)\n",
        "ax=sns.barplot(x=\"F1_Score\", y=\"Model\", data=compare, palette=\"Blues_d\")\n",
        "labels(ax)\n",
        "\n",
        "\n",
        "plt.subplot(313)\n",
        "compare = compare.sort_values(by=\"Average_Precision_Score\", ascending=False)\n",
        "ax=sns.barplot(x=\"Average_Precision_Score\", y=\"Model\", data=compare, palette=\"Blues_d\")\n",
        "labels(ax)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVjW_XLyyabd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXU-Yov1yabd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoeKmJwEyabe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfUL4ZaXCV4A"
      },
      "source": [
        "### Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtY0hTdRCV4A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epg2QSwryabf"
      },
      "source": [
        "___\n",
        "\n",
        "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
        "\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}